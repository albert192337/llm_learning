[Transformer 1. Attention中的Q，K，V是什么 - 知乎](https://zhuanlan.zhihu.com/p/441459022)

[缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 知乎](https://zhuanlan.zhihu.com/p/700588653)