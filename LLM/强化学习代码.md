Okay, 这是一个使用 Python 和 PyTorch 实现的深度 Q 网络 (Deep Q-Network, DQN) 强化学习框架的完整示例。DQN 是一种经典且重要的基于价值的强化学习算法，特别适用于具有离散动作空间的环境。

这个框架包含了以下关键组件：

1.  **Q-Network (Q 网络)**：一个神经网络，用于估计在给定状态下执行每个动作的 Q 值（预期累积回报）。
2.  **Replay Buffer (经验回放缓冲区)**：存储过去的经验（状态、动作、奖励、下一个状态、完成标志），用于打破数据相关性并重用经验进行训练。
3.  **DQN Agent (DQN 智能体)**：包含 Q 网络、目标网络（用于稳定训练）、经验回放缓冲区，并负责选择动作（使用 ε-greedy 策略进行探索/利用）和学习（更新 Q 网络权重）。
4.  **Training Loop (训练循环)**：智能体与环境交互的主循环，收集经验，并定期调用智能体的学习方法。

我们将使用 `gymnasium`（OpenAI Gym 的一个维护分支）作为环境库。如果你还没有安装，请先安装：
`pip install torch gymnasium`

```python
# -------------------------------------
# 导入必要的库
# -------------------------------------
import torch
import torch.nn as nn               # PyTorch 神经网络模块
import torch.optim as optim         # PyTorch 优化器模块
import torch.nn.functional as F     # PyTorch 函数式接口 (如激活函数)

import gymnasium as gym             # 强化学习环境库 (OpenAI Gym 的后续版本)
import numpy as np                  # 用于数值计算
import random                       # 用于随机选择 (例如 ε-greedy 策略)
from collections import deque, namedtuple # deque 用于实现高效的回放缓冲区，namedtuple 用于组织经验

# -------------------------------------
# 设置设备 (GPU 或 CPU)
# -------------------------------------
# 检查是否有可用的 CUDA (NVIDIA GPU) 设备，否则使用 CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# -------------------------------------
# 定义 Q 网络结构
# -------------------------------------
class QNetwork(nn.Module):
    """神经网络模型，用于逼近 Q 函数 Q(s, a)"""
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        """
        初始化 Q 网络。
        Args:
            state_dim (int): 输入状态空间的维度。
            action_dim (int): 输出动作空间的维度 (对于离散动作空间，即动作的数量)。
            hidden_dim (int): 隐藏层的神经元数量。
        """
        super(QNetwork, self).__init__() # 调用父类 nn.Module 的初始化方法
        self.fc1 = nn.Linear(state_dim, hidden_dim) # 第一个全连接层 (输入层 -> 隐藏层)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim) # 第二个全连接层 (隐藏层 -> 隐藏层)
        self.fc3 = nn.Linear(hidden_dim, action_dim) # 第三个全连接层 (隐藏层 -> 输出层)

    def forward(self, state):
        """
        定义网络的前向传播路径。
        Args:
            state (torch.Tensor): 输入的状态张量。
        Returns:
            torch.Tensor: 输出每个动作对应的 Q 值。
        """
        x = F.relu(self.fc1(state))  # 通过第一个全连接层，并应用 ReLU 激活函数
        x = F.relu(self.fc2(x))  # 通过第二个全连接层，并应用 ReLU 激活函数
        q_values = self.fc3(x)     # 通过输出层，得到 Q 值 (注意：输出层通常不加激活函数)
        return q_values

# -------------------------------------
# 定义经验回放缓冲区
# -------------------------------------
# 使用 namedtuple 定义一个经验单元的结构，使其更具可读性
Experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])

class ReplayBuffer:
    """固定大小的缓冲区，用于存储和采样经验元组。"""
    def __init__(self, capacity):
        """
        初始化回放缓冲区。
        Args:
            capacity (int): 缓冲区的最大容量。
        """
        # 使用双端队列 (deque) 实现，当达到最大容量时，旧的经验会自动被移除
        self.memory = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """
        将一个经验元组添加到缓冲区中。
        Args:
            state: 当前状态。
            action: 执行的动作。
            reward:获得的奖励。
            next_state: 下一个状态。
            done (bool): 是否结束的标志。
        """
        # 创建一个 Experience 对象
        e = Experience(state, action, reward, next_state, done)
        self.memory.append(e) # 添加到 deque 中

    def sample(self, batch_size):
        """
        从缓冲区中随机采样一批经验。
        Args:
            batch_size (int): 需要采样的经验数量。
        Returns:
            tuple: 包含状态、动作、奖励、下一状态、完成标志的张量元组。
                   例如 (states_tensor, actions_tensor, rewards_tensor, ...)
        """
        # 从 memory 中随机抽取 batch_size 个经验
        experiences = random.sample(self.memory, k=batch_size)

        # 将一批 Experience 对象解构成单独的列表
        # zip(*experiences) 会将 [(s1,a1,r1,..), (s2,a2,r2,..)] 转换成
        # [(s1,s2,..), (a1,a2,..), (r1,r2,..), ...]
        states, actions, rewards, next_states, dones = zip(*experiences)

        # 将 Python 列表或 NumPy 数组转换为 PyTorch 张量，并移动到指定设备
        # 注意数据类型的转换 (float32 用于状态和奖励，long 用于动作索引)
        states = torch.from_numpy(np.vstack(states)).float().to(device)
        actions = torch.from_numpy(np.vstack(actions)).long().to(device) # 动作通常是整数索引
        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)
        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)
        # dones 需要转换为 float 类型以便在计算目标 Q 值时使用 (True -> 1.0, False -> 0.0)
        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)

        return (states, actions, rewards, next_states, dones)

    def __len__(self):
        """返回缓冲区中当前存储的经验数量。"""
        return len(self.memory)

# -------------------------------------
# 定义 DQN 智能体
# -------------------------------------
class DQNAgent:
    """实现 DQN 算法的智能体。"""
    def __init__(self, state_dim, action_dim,
                 learning_rate=1e-4,       # 学习率
                 gamma=0.99,             # 折扣因子 γ
                 epsilon_start=1.0,      # ε-greedy 策略的初始 ε 值
                 epsilon_end=0.01,       # ε-greedy 策略的最终 ε 值
                 epsilon_decay=0.995,    # ε 值的衰减率
                 buffer_size=10000,      # 经验回放缓冲区的容量
                 batch_size=64,          # 训练时的小批量大小
                 target_update_freq=100): # 目标网络更新频率 (多少步更新一次)
        """
        初始化 DQN 智能体。
        Args:
            state_dim (int): 状态空间维度。
            action_dim (int): 动作空间维度。
            learning_rate (float): 优化器的学习率。
            gamma (float): 折扣因子，用于计算未来奖励的当前价值。
            epsilon_start (float): ε-greedy 探索策略的起始 ε 值。
            epsilon_end (float): ε-greedy 探索策略的最终 ε 值。
            epsilon_decay (float): 每次学习后 ε 的衰减因子。
            buffer_size (int): 经验回放缓冲区的容量。
            batch_size (int): 从缓冲区采样进行训练的小批量大小。
            target_update_freq (int): 每隔多少次学习步骤更新一次目标网络。
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq

        # --- 网络 ---
        # Q 网络 (策略网络): 用于选择动作和计算当前 Q 值，会被训练更新
        self.q_network = QNetwork(state_dim, action_dim).to(device)
        # 目标网络: 用于计算目标 Q 值，其权重定期从 Q 网络复制，以稳定训练
        self.target_network = QNetwork(state_dim, action_dim).to(device)
        # 初始化时，将 Q 网络的权重复制到目标网络
        self.target_network.load_state_dict(self.q_network.state_dict())
        # 将目标网络设置为评估模式，因为我们只用它来计算目标值，不进行训练
        self.target_network.eval()

        # --- 优化器 ---
        # 使用 Adam 优化器来更新 Q 网络的参数
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # --- 经验回放 ---
        self.memory = ReplayBuffer(buffer_size)

        # --- 学习步骤计数器 ---
        # 用于跟踪调用 learn() 方法的次数，以决定何时更新目标网络
        self.learn_step_counter = 0

    def select_action(self, state, evaluation_mode=False):
        """
        根据当前状态和 ε-greedy 策略选择一个动作。
        Args:
            state (np.ndarray): 当前环境状态。
            evaluation_mode (bool): 是否处于评估模式。如果是，则不进行探索 (epsilon=0)。
        Returns:
            int: 选择的动作的索引。
        """
        # ε-greedy 策略
        # 以 ε 的概率随机选择一个动作 (探索)
        if not evaluation_mode and random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        # 以 1-ε 的概率选择 Q 值最大的动作 (利用)
        else:
            # 1. 将 NumPy 状态转换为 PyTorch 张量，并添加一个批次维度 (unsqueeze(0))
            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)
            # 2. 将 Q 网络设置为评估模式 (可选，但好习惯，因为它会禁用 dropout 等)
            self.q_network.eval()
            # 3. 使用 torch.no_grad() 禁用梯度计算，因为这里只是前向传播获取 Q 值，不需要计算梯度
            with torch.no_grad():
                action_values = self.q_network(state_tensor)
            # 4. 将 Q 网络恢复为训练模式
            self.q_network.train()
            # 5. 选择具有最大 Q 值的动作的索引
            # action_values 是一个形状为 [1, action_dim] 的张量
            # .max(1) 返回沿维度 1 (动作维度) 的最大值和对应的索引
            # [1] 取出索引，.item() 将张量索引转换为 Python 整数
            return action_values.max(1)[1].item()

    def learn(self):
        """
        从经验回放缓冲区采样一批数据，计算损失并更新 Q 网络。
        同时处理 ε 的衰减和目标网络的更新。
        """
        # 1. 检查缓冲区是否有足够的经验进行采样
        if len(self.memory) < self.batch_size:
            return # 如果经验不足，则不进行学习

        # 2. 从缓冲区采样一批经验
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        # --- 3. 计算目标 Q 值 (y_i) ---
        # 使用目标网络计算下一状态的最大 Q 值: Q_target(s', argmax_a' Q(s', a'))
        # .max(1)[0] 获取每个样本在下一状态下所有动作的最大 Q 值
        # .unsqueeze(1) 将其形状从 [batch_size] 调整为 [batch_size, 1] 以匹配其他张量
        # .detach() 防止梯度流向目标网络，因为我们不更新目标网络的参数
        next_q_values_target = self.target_network(next_states).max(1)[0].unsqueeze(1).detach()

        # 计算 TD 目标: r + γ * max_a' Q_target(s', a')
        # 如果一个状态是终止状态 (done=1)，则其未来价值为 0，所以乘以 (1 - dones)
        target_q_values = rewards + (self.gamma * next_q_values_target * (1 - dones))

        # --- 4. 计算当前 Q 值 (Q(s, a)) ---
        # 使用 Q 网络计算当前状态下所有动作的 Q 值
        q_values_all = self.q_network(states)
        # 使用 gather() 方法根据实际采取的动作 (actions) 从 q_values_all 中提取对应的 Q 值
        # actions 张量的形状需要是 [batch_size, 1] 以匹配 gather 的要求
        current_q_values = q_values_all.gather(1, actions)

        # --- 5. 计算损失 ---
        # 使用均方误差 (MSE) 损失函数计算当前 Q 值和目标 Q 值之间的差异
        # loss = F.mse_loss(current_q_values, target_q_values)
        # 或者使用 Huber 损失 (Smooth L1 Loss)，它对异常值不那么敏感
        loss = F.smooth_l1_loss(current_q_values, target_q_values)

        # --- 6. 优化模型 ---
        self.optimizer.zero_grad() # 清除之前的梯度
        loss.backward()          # 反向传播计算梯度
        # 可选：梯度裁剪，防止梯度爆炸 (尤其在 RNN 或复杂任务中)
        # for param in self.q_network.parameters():
        #     param.grad.data.clamp_(-1, 1)
        self.optimizer.step()      # 根据梯度更新 Q 网络的参数

        # --- 7. 更新 ε ---
        # 线性衰减或指数衰减 ε 值
        self._decay_epsilon()

        # --- 8. 定期更新目标网络 ---
        self.learn_step_counter += 1
        if self.learn_step_counter % self.target_update_freq == 0:
            self._update_target_network()

    def _update_target_network(self):
        """将 Q 网络的权重复制到目标网络。"""
        print(f"Updating target network at step {self.learn_step_counter}...")
        self.target_network.load_state_dict(self.q_network.state_dict())

    def _decay_epsilon(self):
        """衰减 ε 值，使其不低于设定的最小值。"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

    def save_model(self, filepath):
        """保存 Q 网络的权重。"""
        torch.save(self.q_network.state_dict(), filepath)
        print(f"Model saved to {filepath}")

    def load_model(self, filepath):
        """加载 Q 网络的权重。"""
        self.q_network.load_state_dict(torch.load(filepath, map_location=device))
        # 加载模型后，也应该更新目标网络
        self.target_network.load_state_dict(self.q_network.state_dict())
        # 加载模型后，通常用于评估，将网络设为评估模式
        self.q_network.eval()
        self.target_network.eval()
        print(f"Model loaded from {filepath}")


# -------------------------------------
# 训练函数
# -------------------------------------
def train(env_name='CartPole-v1', num_episodes=500, print_every=50):
    """
    训练 DQN 智能体的主函数。
    Args:
        env_name (str): Gymnasium 环境的名称。
        num_episodes (int): 训练的总回合数。
        print_every (int): 每隔多少回合打印一次训练信息。
    """
    # 1. 创建环境
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0] # 获取状态维度
    action_dim = env.action_space.n          # 获取动作数量 (离散)

    print(f"Environment: {env_name}")
    print(f"State dimension: {state_dim}")
    print(f"Action dimension: {action_dim}")

    # 2. 创建 DQN 智能体
    agent = DQNAgent(state_dim, action_dim,
                     learning_rate=5e-4,      # 学习率可以调整
                     gamma=0.99,
                     epsilon_start=1.0,
                     epsilon_end=0.05,       # 最终 epsilon 可以稍高，保留一点探索
                     epsilon_decay=0.99,     # 衰减率可以调整
                     buffer_size=50000,      # 增大 buffer 容量
                     batch_size=128,         # 增大 batch size
                     target_update_freq=500) # 目标网络更新频率可以调整

    # 3. 训练循环
    episode_rewards = [] # 记录每个回合的总奖励
    recent_scores = deque(maxlen=100) # 记录最近 100 回合的分数，用于判断是否解决

    for i_episode in range(1, num_episodes + 1):
        # 重置环境，获取初始状态
        # Gymnasium 返回 (状态, 附加信息)，我们只需要状态
        state, info = env.reset()
        episode_reward = 0
        terminated = False # 标记是否达到终止状态 (如杆子倒下)
        truncated = False  # 标记是否达到截断条件 (如达到最大步数)
        max_steps_per_episode = 1000 # 设置一个较大的每回合最大步数
        steps = 0

        # 每个回合的循环
        while not terminated and not truncated and steps < max_steps_per_episode:
            # 1. 选择动作
            action = agent.select_action(state)

            # 2. 与环境交互
            # Gymnasium 返回 (下一状态, 奖励, 终止标志, 截断标志, 附加信息)
            next_state, reward, terminated, truncated, info = env.step(action)

            # 重要：修改奖励以鼓励存活 (可选，但对 CartPole 有帮助)
            # if terminated and steps < max_steps_per_episode -1: # 如果是因为失败而终止
            #     reward = -10.0 # 给予一个较大的负奖励
            # else:
            #     reward = 0.1 # 每存活一步给予一个小正奖励 (或者保持原始奖励 1.0)


            # 3. 存储经验到回放缓冲区
            # 将 done 标志合并：如果 terminated 或 truncated，都认为该步是 "结束"
            done = terminated or truncated
            agent.memory.push(state, action, reward, next_state, done)

            # 4. 更新状态
            state = next_state
            episode_reward += reward
            steps += 1

            # 5. 进行学习 (从缓冲区采样并更新网络)
            # 只有当缓冲区中有足够的数据时，agent.learn() 内部才会执行更新
            agent.learn()

        # --- 回合结束 ---
        episode_rewards.append(episode_reward)
        recent_scores.append(episode_reward)
        average_score = np.mean(recent_scores) # 计算最近 100 回合的平均得分

        # 打印训练信息
        if i_episode % print_every == 0:
            print(f"Episode {i_episode}/{num_episodes} | Reward: {episode_reward:.2f} | "
                  f"Avg Reward (Last 100): {average_score:.2f} | Epsilon: {agent.epsilon:.3f}")

        # 检查是否解决了环境 (CartPole-v1 的解决标准通常是最近 100 回合平均奖励 >= 475)
        # 注意: Gymnasium v0.26 后的 CartPole-v1 默认最大步数是 500，奖励是每步+1
        # 如果使用修改后的奖励，解决标准可能需要调整
        if average_score >= 475.0 and env_name == 'CartPole-v1':
             print(f"\nEnvironment solved in {i_episode} episodes! Average score: {average_score:.2f}")
             # 可以选择在这里保存模型并提前结束训练
             # agent.save_model(f"{env_name}_dqn_solved.pth")
             # break

    # 训练结束后关闭环境
    env.close()
    print("\nTraining finished.")

    # 可以选择在这里保存最终模型
    agent.save_model(f"{env_name}_dqn_final.pth")

    # 返回训练过程中的奖励记录，方便后续绘图分析
    return episode_rewards

# -------------------------------------
# 运行训练
# -------------------------------------
if __name__ == "__main__":
    # 设置环境名称，常见的有 'CartPole-v1', 'LunarLander-v2' (需要 pip install gymnasium[box2d])
    environment_name = 'CartPole-v1'
    total_episodes = 1000  # 训练回合数可以调整
    print_interval = 20   # 打印间隔可以调整

    rewards_history = train(env_name=environment_name,
                            num_episodes=total_episodes,
                            print_every=print_interval)

    # (可选) 绘制奖励曲线
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 5))
        plt.plot(rewards_history)
        # 计算移动平均线，使曲线更平滑
        moving_avg = np.convolve(rewards_history, np.ones(100)/100, mode='valid')
        plt.plot(np.arange(len(moving_avg)) + 99, moving_avg, label='Moving Average (100 episodes)')
        plt.title(f'Episode Rewards over Time ({environment_name})')
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.legend()
        plt.grid(True)
        plt.show()
    except ImportError:
        print("\nMatplotlib not found. Skipping reward plot. Install with: pip install matplotlib")

```

**代码详细注释说明:**

1.  **导入库**: 导入了 `torch` 相关的神经网络、优化器、函数，`gymnasium` 用于环境交互，`numpy` 用于数值操作，`random` 用于随机性，`collections.deque` 用于高效实现缓冲区，`namedtuple` 用于清晰地表示经验。
2.  **设置设备**: 自动检测并设置使用 GPU (如果可用) 或 CPU。将张量和模型移到正确的设备上对于性能至关重要。
3.  **QNetwork 类**:
    *   继承自 `torch.nn.Module`，这是所有 PyTorch 模型的基类。
    *   `__init__`: 定义网络的层。这里使用了三个全连接层 (`nn.Linear`) 和 ReLU 激活函数 (`F.relu`) 构建了一个简单的多层感知机 (MLP)。输入维度是状态维度，输出维度是动作数量。
    *   `forward`: 定义数据通过网络时的计算流程。输入状态 `state`，输出每个动作对应的 Q 值。
4.  **Experience 元组**: 使用 `namedtuple` 定义了一个 `Experience` 结构，使代码更易读，访问经验字段（如 `exp.state`, `exp.action`）比使用索引更清晰。
5.  **ReplayBuffer 类**:
    *   `__init__`: 使用 `deque` 创建一个有固定最大长度 (`capacity`) 的队列。当队列满时，添加新元素会自动移除最旧的元素。
    *   `push`: 将一个 `Experience` 对象添加到 `memory` 队列中。
    *   `sample`: 从 `memory` 中随机抽取 `batch_size` 个经验。关键步骤是将抽取的经验列表（列表中的每个元素是一个 `Experience` 对象）解构成单独的状态、动作、奖励等列表，然后使用 `torch.from_numpy` 将它们转换成 PyTorch 张量，并确保它们位于正确的 `device` 上。注意数据类型的转换（`float` 用于连续值，`long` 用于离散索引如动作）。`dones` 标志被转换为 `float`，以便在计算目标 Q 值时进行乘法运算 (`True` -> 1.0, `False` -> 0.0)。
    *   `__len__`: 返回当前缓冲区中的经验数量。
6.  **DQNAgent 类**:
    *   `__init__`:
        *   存储状态和动作维度、超参数（学习率 `learning_rate`、折扣因子 `gamma`、ε-greedy 相关参数 `epsilon_start`, `epsilon_end`, `epsilon_decay`、缓冲区大小 `buffer_size`、批处理大小 `batch_size`、目标网络更新频率 `target_update_freq`）。
        *   创建 **两个** `QNetwork` 实例：`q_network` (策略网络) 和 `target_network` (目标网络)。这是 DQN 的关键，使用目标网络计算目标 Q 值可以提高训练稳定性。
        *   将 `q_network` 的初始权重复制到 `target_network`。
        *   将 `target_network` 设置为评估模式 (`eval()`)，因为它只用于前向传播计算目标值，不需要训练或 Dropout 等。
        *   创建 `Adam` 优化器，只优化 `q_network` 的参数。
        *   创建 `ReplayBuffer` 实例。
        *   初始化 `learn_step_counter`，用于跟踪学习步数以决定何时更新目标网络。
    *   `select_action`:
        *   实现 ε-greedy 策略：以 `epsilon` 的概率随机选择动作（探索），否则根据 `q_network` 预测的 Q 值选择最优动作（利用）。
        *   在利用步骤中，需要将 NumPy 状态转换为 PyTorch 张量，增加批次维度 (`unsqueeze(0)`)，移到 `device`。
        *   使用 `torch.no_grad()` 包裹网络前向传播，因为此步骤不涉及训练，可以节省计算资源并防止不必要的梯度计算。
        *   `.max(1)[1].item()` 用于获取 Q 值最大的动作的索引。
    *   `learn`:
        *   检查缓冲区大小，足够时才进行学习。
        *   从 `memory` 采样一个批次的经验。
        *   **计算目标 Q 值**:
            *   使用 `target_network` 计算 `next_states` 的 Q 值。
            *   `.max(1)[0]` 获取每个下一状态的最大 Q 值。
            *   `.detach()` **非常重要**，它阻止了梯度流向 `target_network` 的参数。我们只希望根据这个目标值来更新 `q_network`。
            *   应用 TD 公式计算目标：`reward + gamma * max_next_q * (1 - done)`。`(1 - done)` 确保如果 `next_state` 是终止状态，其未来价值为 0。
        *   **计算当前 Q 值**:
            *   使用 `q_network` 计算 `states` 的所有动作的 Q 值。
            *   使用 `.gather(1, actions)` 从所有 Q 值中精确地提取出与样本中实际执行的 `actions` 相对应的 Q 值。
        *   **计算损失**: 使用 `F.smooth_l1_loss` (Huber 损失) 或 `F.mse_loss` 计算当前 Q 值和目标 Q 值之间的差异。Huber 损失对异常值更鲁棒。
        *   **优化**: 执行标准的 PyTorch 优化步骤：梯度清零 (`optimizer.zero_grad()`)、反向传播 (`loss.backward()`)、更新参数 (`optimizer.step()`)。
        *   **更新 Epsilon**: 调用 `_decay_epsilon` 方法来减小 `epsilon` 值，逐渐从探索转向利用。
        *   **更新目标网络**: 增加 `learn_step_counter`，如果达到 `target_update_freq`，则调用 `_update_target_network` 方法。
    *   `_update_target_network`: 将 `q_network` 的当前权重 (`state_dict`) 复制到 `target_network`。
    *   `_decay_epsilon`: 按照 `epsilon_decay` 因子指数衰减 `epsilon` 值，但不低于 `epsilon_end`。
    *   `save_model`/`load_model`: 提供保存和加载网络权重的功能，方便之后继续训练或评估。
7.  **train 函数**:
    *   设置环境 (`gym.make`)，获取状态和动作维度。
    *   创建 `DQNAgent` 实例。
    *   初始化记录列表 (`episode_rewards`, `recent_scores`)。
    *   **主训练循环** (`for i_episode in range(...)`)：
        *   每个回合开始时重置环境 (`env.reset()`)。
        *   **回合内循环** (`while not terminated and not truncated ...`)：
            *   智能体选择动作 (`agent.select_action`)。
            *   环境执行动作 (`env.step`)，返回结果。
            *   将经验 (`state`, `action`, `reward`, `next_state`, `done`) 存入回放缓冲区 (`agent.memory.push`)。`done` 标志合并了 `terminated` 和 `truncated`。
            *   更新当前状态 (`state = next_state`)。
            *   调用 `agent.learn()` 进行学习（如果缓冲区满足条件）。
            *   累积回合奖励，增加步数计数器。
        *   回合结束后，记录总奖励，更新最近得分，计算平均分。
        *   定期打印训练进度和指标。
        *   检查是否达到环境的解决标准（可选）。
    *   训练结束后关闭环境 (`env.close()`)。
    *   保存最终模型（可选）。
    *   返回奖励历史记录。
8.  **主程序入口 (`if __name__ == "__main__":`)**:
    *   设置环境名称、训练回合数等参数。
    *   调用 `train` 函数开始训练。
    *   （可选）使用 `matplotlib` 绘制每个回合奖励的变化曲线，以及移动平均线，以可视化训练过程。

这个框架提供了一个功能齐全、注释详细的 DQN 实现，你可以基于此进行修改和扩展，例如尝试不同的环境、调整超参数、改进网络结构或实现更高级的 DQN 变种（如 Double DQN, Dueling DQN）。